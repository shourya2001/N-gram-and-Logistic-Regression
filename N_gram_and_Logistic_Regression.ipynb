{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: n-Gram Language Models"
      ],
      "metadata": {
        "id": "lOHLpU5y0Wgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libaries\n",
        "import requests\n",
        "import collections\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "KQ0G021VWiOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for invalid HTTP status codes\n",
        "text_data = response.text"
      ],
      "metadata": {
        "id": "Ya2CfD3vWk2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "random.seed(42)\n",
        "\n",
        "pos = random.randint(0, len(text_data) - 1000)\n",
        "print(text_data[pos:pos+100])"
      ],
      "metadata": {
        "id": "HtJfs33cWmBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c416c81e-ea47-435b-fe52-cd7006ac16aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BY:\n",
            "Many good morrows to my noble lord!\n",
            "\n",
            "HASTINGS:\n",
            "Good morrow, Catesby; you are early stirring\n",
            "What\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing - do not change\n",
        "def preprocess_text(text_data):\n",
        "  text_data = text_data.replace(',',' , ').replace(';', ' ').replace(':', ' ').replace('.',' . ').replace('?',' ? ').replace('!',' ! ')\n",
        "  text_data = text_data.replace('-', ' ')\n",
        "  text_data = text_data.replace('\\'', '').replace('\"', '')\n",
        "  text_data = text_data.replace('  ', ' ')\n",
        "  text_data = text_data.replace('\\n\\n','\\n').replace('\\n',' </s> <s> ')\n",
        "  text_data = '<s> ' + text_data + ' </s>'\n",
        "  text_data = text_data.lower()\n",
        "  return text_data\n",
        "\n",
        "text_data = preprocess_text(response.text)\n",
        "print(f\"Number of words: {len(text_data.split(' '))}\")"
      ],
      "metadata": {
        "id": "SaPIqFwHWpA3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ad8c05-dac1-4aab-a287-e7cc3a9536d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words: 328097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = text_data[:-10_000]\n",
        "test_data = text_data[-10_000:]\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "cnHQ-enjWqVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f08a431-6045-40da-fefa-37aab7af5135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1431030, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(train_data.split(' '))\n",
        "print(f\"Number of unique words: {len(vocab)}\")\n",
        "print(f\"Sample unique words: {list(vocab)[:10]}\")"
      ],
      "metadata": {
        "id": "IrYnPsMKWrvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919d9f82-c75b-4ee0-be63-1ba7a2f6fa62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 12124\n",
            "Sample unique words: ['', 'perdona', 'fosset', 'impossibilities', 'chronicle', 'induction', 'sward', 'garners', 'darts', 'apology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## A. Dealing with Out of Vocabulary Words"
      ],
      "metadata": {
        "id": "9YO9DwrkWuQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_oov_words(corpus, n=3):\n",
        "    \"\"\"\n",
        "    Identify out-of-vocabulary (OOV) words that appear less than `n` times in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The dataset to process. It should be a dictionary with a 'text' key.\n",
        "    - n: The frequency threshold below which words are considered OOV.\n",
        "\n",
        "    Returns:\n",
        "    - A set of out-of-vocabulary words.\n",
        "    \"\"\"\n",
        "    count = collections.Counter(corpus.split(' '))\n",
        "    OOV = set()\n",
        "    for word in count:\n",
        "        if count[word]<3:\n",
        "            OOV.add(word)\n",
        "    return OOV\n",
        "\n"
      ],
      "metadata": {
        "id": "CzuEDZoZWs7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oov_words = identify_oov_words(train_data)\n",
        "\n",
        "vocab = vocab - oov_words\n",
        "vocab.add('<UNK>')\n",
        "print(f\"Number of OOV words: {len(oov_words)}\")\n",
        "print(f\"Expected number of OOV words: {7181}\")\n",
        "\n",
        "assert len(oov_words) == 7181"
      ],
      "metadata": {
        "id": "S3_q0smRW2Ex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e82af14-4ed7-4dda-f051-a8dfaadc4f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of OOV words: 7181\n",
            "Expected number of OOV words: 7181\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = ' '.join(['<UNK>' if word not in vocab else word for word in train_data.split(' ')])\n",
        "test_data = ' '.join(['<UNK>' if word not in vocab else word for word in test_data.split(' ')])"
      ],
      "metadata": {
        "id": "L60G-KIEW5KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Create the N-Gram Models"
      ],
      "metadata": {
        "id": "LHKNJJ-gW6qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uni_counts = collections.defaultdict(lambda:0)\n",
        "bi_counts = collections.defaultdict(lambda:0)\n",
        "tri_counts = collections.defaultdict(lambda:0)\n",
        "four_counts = collections.defaultdict(lambda:0)\n",
        "five_counts = collections.defaultdict(lambda:0)"
      ],
      "metadata": {
        "id": "s6p1troiW9lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_counts(corpus):\n",
        "    data = corpus.split(' ')\n",
        "    for i in range(len(data)):\n",
        "        uni_counts[data[i]] += 1\n",
        "        if i < (len(data) - 1):\n",
        "            bi_counts[tuple(data[i:i+2])] += 1\n",
        "        if i < (len(data) - 2):\n",
        "            tri_counts[tuple(data[i:i+3])] += 1\n",
        "        if i < (len(data) - 3):\n",
        "            four_counts[tuple(data[i:i+4])] += 1\n",
        "        if i < (len(data) - 4):\n",
        "            five_counts[tuple(data[i:i+5])] += 1\n",
        "\n",
        "ngram_counts(train_data)"
      ],
      "metadata": {
        "id": "nY3UMFE6W_ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uni = collections.defaultdict(lambda:0)\n",
        "bi = collections.defaultdict(lambda:0)\n",
        "tri = collections.defaultdict(lambda:0)\n",
        "four = collections.defaultdict(lambda:0)\n",
        "five = collections.defaultdict(lambda:0)"
      ],
      "metadata": {
        "id": "-aACGp05XAVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ngram_probabilities():\n",
        "\n",
        "    for word, count in uni_counts.items():\n",
        "        uni[word] = count / sum(uni_counts.values())\n",
        "\n",
        "    for bigram, count in bi_counts.items():\n",
        "        prev_word = bigram[0]\n",
        "        bi[bigram] = count / uni_counts[prev_word]\n",
        "\n",
        "    for trigram, count in tri_counts.items():\n",
        "        prev_bigram = tuple(trigram[:2])\n",
        "        tri[trigram] = count / bi_counts[prev_bigram]\n",
        "\n",
        "    for fourgram, count in four_counts.items():\n",
        "        prev_trigram = tuple(fourgram[:3])\n",
        "        four[fourgram] = count / tri_counts[prev_trigram]\n",
        "\n",
        "    for fivegram, count in five_counts.items():\n",
        "        prev_fourgram = tuple(fivegram[:4])\n",
        "        five[fivegram] = count / four_counts[prev_fourgram]\n",
        "\n",
        "compute_ngram_probabilities()\n"
      ],
      "metadata": {
        "id": "CBnpHlliY3wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluation\n",
        "# assert five[('<s>', 'against', 'the', 'roman', 'state')] == 1.0 # prob of last given prev 4\n",
        "# assert four[('remain', '</s>', '<s>', 'i')] == 0.25 # prob of last given prev 3\n",
        "# assert tri[('did', 'see', 'and')] == 0.5 # prob of last given prev 2\n",
        "# assert bi[('rash', 'like')] == 0.1 # prob of last given prev 1\n",
        "# assert round(uni[('citizen')],5) == 0.00031 # prob of last"
      ],
      "metadata": {
        "id": "CUgTaWzVXDsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Interpolation Smoothing"
      ],
      "metadata": {
        "id": "Le7l1pSAXGWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_bigram_probability_with_smoothing(word1, word2):\n",
        "    # INSERT CODE HERE\n",
        "    bigram_count = bi_counts.get((word1, word2), 0)\n",
        "    unigram_count = uni_counts.get(word1, 0)\n",
        "    vocab_size = len(uni_counts)\n",
        "    probability = (bigram_count + 1) / (unigram_count + vocab_size)\n",
        "\n",
        "    return probability"
      ],
      "metadata": {
        "id": "sVXNg0bZXLfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Evaluate Perplexity"
      ],
      "metadata": {
        "id": "p70s9JZcXQIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(data):\n",
        "    \"\"\"\n",
        "    Computes the perplexity of a given text data using a bigram language model.\n",
        "\n",
        "    Parameters:\n",
        "    - data : str\n",
        "    Returns:\n",
        "    - float\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(data.split(' ')) >= 5\n",
        "    words = data.split(' ')\n",
        "    N = len(words)\n",
        "\n",
        "    log_prob_sum = 0\n",
        "\n",
        "    for i in range(N - 1):\n",
        "        w1 = words[i]\n",
        "        w2 = words[i+1]\n",
        "        prob = calculate_bigram_probability_with_smoothing(w1, w2)\n",
        "        log_prob_sum += math.log(prob)\n",
        "\n",
        "    perplexity = math.exp(-log_prob_sum / (N))\n",
        "\n",
        "    return perplexity\n",
        "\n"
      ],
      "metadata": {
        "id": "pqODu8L-XPS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert round(compute_perplexity(test_data)) == 129"
      ],
      "metadata": {
        "id": "vVLFr2EfXWOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "c-evK15Gwf3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this question, you will be guided to implement logistic regression classifer from scratch. You will use LR classifer to do sentiment analysis task on Twitter dataset (the dataset is provided in the code)."
      ],
      "metadata": {
        "id": "1iiTuJWnfhA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "M7etuJ5pwymq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import twitter_samples\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "Tw63O_jqwfJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "O8KazUgww7rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data\n"
      ],
      "metadata": {
        "id": "0zmbOD3-xgpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "lq8Rr5R4xc8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
      ],
      "metadata": {
        "id": "pd-qM1SBxlQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg"
      ],
      "metadata": {
        "id": "SJsYMwnoxodO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine positive and negative labels\n",
        "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
        "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
      ],
      "metadata": {
        "id": "CVEhgGkix0UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.  Text processing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Remove old style retweet with 'RT' in the sentence\n",
        "*   Remove hyperlinks\n",
        "\n",
        "*   Remove hashtag\n",
        "*   Tokenize the sentence using TweetTokenizer\n",
        "\n",
        "\n",
        "*   Remove stop words\n",
        "*   Use PorterStemmer to create stem of words in tweet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vqd8BjfkxDaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "def process_tweet(tweet):\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "    tokenizer = TweetTokenizer(preserve_case=False)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    tweet_tokens = [word for word in tweet_tokens if word not in stop_words]\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    tweet_tokens = [stemmer.stem(word) for word in tweet_tokens]\n",
        "\n",
        "    return tweet_tokens"
      ],
      "metadata": {
        "id": "Cp6zvJ3uw-rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a function that will take tweets and their labels as input, go through every tweet, preprocess them, count the occurrence of every word in the data set and create a frequency dictionary.\n",
        "\n",
        "Notice how the outer for loop goes through each tweet, and the inner for loop steps through each word in a tweet.\n",
        "The freqs dictionary is the frequency dictionary that's being built.\n",
        "The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0). The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
      ],
      "metadata": {
        "id": "SeW_hTjhxTQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    freqs = collections.defaultdict(int)\n",
        "\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        processed_tweet = process_tweet(tweet)\n",
        "        for word in processed_tweet:\n",
        "            freqs[(word, int(ys[i]))] += 1\n",
        "\n",
        "    return freqs\n"
      ],
      "metadata": {
        "id": "L-s8dg8NxX4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Logistic regression\n",
        "\n",
        "\n",
        "### Sigmoid\n",
        "\n"
      ],
      "metadata": {
        "id": "A5iTSkQHx6I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sigmoid(z):\n",
        "    '''\n",
        "    Input:\n",
        "        z: is the input (can be a scalar or an array)\n",
        "    Output:\n",
        "        h: the sigmoid of z\n",
        "    '''\n",
        "    return 1/(1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "WP5U6cRDyFqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(x, y, theta):\n",
        "    m = len(y)\n",
        "    z = np.dot(x, theta)\n",
        "    h = sigmoid(z)\n",
        "\n",
        "    J = -(1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y).T, np.log(1 - h)))\n",
        "    return J[0]\n",
        "\n",
        "def gradientDescent(x, y, theta, alpha, num_iters):\n",
        "    '''\n",
        "    Input:\n",
        "        x: matrix of features which is (m,n+1)\n",
        "        y: corresponding labels of the input matrix x, dimensions (m,1)\n",
        "        theta: weight vector of dimension (n+1,1)\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations you want to train your model for\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: your final weight vector\n",
        "    Hint: you might want to print the cost to make sure that it is going down.\n",
        "    '''\n",
        "    m = len(y)\n",
        "    epsilon = 1e-5\n",
        "    J = 0\n",
        "\n",
        "    for i in range(num_iters):\n",
        "        h = sigmoid(np.dot(x, theta))\n",
        "        error = h - y\n",
        "        gradient = (1/m) * np.dot(x.T, error)\n",
        "        theta = theta - alpha * gradient\n",
        "        J = -(1/m) * (np.dot(y.T, np.log(h + epsilon)) + np.dot((1 - y).T, np.log(1 - h + epsilon)))\n",
        "        # J_history.append(J)\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Cost: {J[0][0]}\")\n",
        "\n",
        "    return J, theta"
      ],
      "metadata": {
        "id": "u5ylWyKIzEqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Extracting the features\n",
        "\n",
        "* Given a list of tweets, extract the features and store them in a matrix. We will extract two features.\n",
        "    * The first feature is the number of positive words in a tweet.\n",
        "    * The second feature is the number of negative words in a tweet.\n",
        "* Then train logistic regression classifier on these features.\n",
        "* Test the classifier on a validation set.\n"
      ],
      "metadata": {
        "id": "kOW5hhOezOnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # 3 elements in the form of a 1 x 3 vector\n",
        "    x = np.zeros((1, 3))\n",
        "\n",
        "    #bias term is set to 1\n",
        "    x[0,0] = 1\n",
        "\n",
        "    # write your code here\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "\n",
        "    for word in word_l:\n",
        "        pos_count += freqs.get((word, 1.0), 0)\n",
        "        neg_count += freqs.get((word, 0.0), 0)\n",
        "\n",
        "    x[0, 1] = pos_count\n",
        "    x[0, 2] = neg_count\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "KosqgShlzIbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Training the Model"
      ],
      "metadata": {
        "id": "ThQX8MJUzYQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_x, train_y, freqs, alpha=0.01, num_iters=1500):\n",
        "    '''\n",
        "    Input:\n",
        "        train_x: a list of training tweets\n",
        "        train_y: a numpy array of corresponding labels (1 = positive, 0 = negative)\n",
        "        freqs: a frequency dictionary of (word, label) pairs\n",
        "        alpha: learning rate\n",
        "        num_iters: number of iterations to train the model\n",
        "    Output:\n",
        "        J: the final cost\n",
        "        theta: the learned weight vector\n",
        "    '''\n",
        "    m = len(train_x)\n",
        "\n",
        "    X = np.zeros((m, 3))\n",
        "\n",
        "    for i in range(m):\n",
        "        X[i, :] = extract_features(train_x[i], freqs)\n",
        "\n",
        "    theta = np.zeros((3, 1))\n",
        "\n",
        "    J, theta = gradientDescent(X, train_y, theta, alpha, num_iters)\n",
        "\n",
        "    print(f\"Final cost: {J}\")\n",
        "    print(f\"Final weights: {theta}\")\n",
        "\n",
        "    return J, theta\n"
      ],
      "metadata": {
        "id": "Bh-KGwH1zXkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## E. Testing your model"
      ],
      "metadata": {
        "id": "dZ3BTrKTziUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, freqs, theta):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "        theta: (3,1) vector of weights\n",
        "    Output:\n",
        "        y_pred: the probability of a tweet being positive or negative\n",
        "    '''\n",
        "    # write your code here\n",
        "    x = extract_features(tweet, freqs)\n",
        "    z = np.dot(x, theta)\n",
        "    y_pred = sigmoid(z)\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "C0gsWj6ozgiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output:\n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "\n",
        "   # write your code here\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for tweet, label in zip(test_x, test_y):\n",
        "        y_pred = predict_tweet(tweet, freqs, theta)\n",
        "        y_hat = 1 if y_pred > 0.5 else 0\n",
        "        if y_hat == int(label):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / len(test_x)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "3U2Cht1Cz437"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = build_freqs(train_x, train_y)\n",
        "J, theta = train_model(train_x, train_y, freqs, alpha=0.01, num_iters=1500)\n",
        "tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "GwaUhFDlz8wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6454855-de62-47d2-f06b-70394aadf7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-bcbc49081965>:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  freqs[(word, int(ys[i]))] += 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Cost: 0.6931271807599442\n",
            "Iteration 100, Cost: 0.17530401828043315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-edf42e1f19d6>:10: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 200, Cost: 0.17399426741432353\n",
            "Iteration 300, Cost: 0.17520711117140692\n",
            "Iteration 400, Cost: 0.17524481816504778\n",
            "Iteration 500, Cost: 0.17519088355289383\n",
            "Iteration 600, Cost: 0.1752999779349184\n",
            "Iteration 700, Cost: 0.17494055392894578\n",
            "Iteration 800, Cost: 0.17342522872189978\n",
            "Iteration 900, Cost: 0.17543401586026822\n",
            "Iteration 1000, Cost: 0.17568868255091752\n",
            "Iteration 1100, Cost: 0.17526173459265482\n",
            "Iteration 1200, Cost: 0.17539699226449101\n",
            "Iteration 1300, Cost: 0.1731496904879286\n",
            "Iteration 1400, Cost: 0.17522097825587518\n",
            "Final cost: [[0.17517855]]\n",
            "Final weights: [[  0.07790017]\n",
            " [  9.29876709]\n",
            " [-11.68433543]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-b9eb3e7da36a>:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  if y_hat == int(label):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.9855\n"
          ]
        }
      ]
    }
  ]
}